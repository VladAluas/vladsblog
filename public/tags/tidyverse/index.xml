<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidyverse on Clarity</title>
    <link>https://www.vladaluas.com/tags/tidyverse/</link>
    <description>Recent content in tidyverse on Clarity</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Mon, 15 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.vladaluas.com/tags/tidyverse/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Cleaning</title>
      <link>https://www.vladaluas.com/post/data-cleaning/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.vladaluas.com/post/data-cleaning/</guid>
      <description>
        
          
&lt;script src=&#34;https://www.vladaluas.com/post/data-cleaning/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;blockquote&gt;
“Like families, tidy datasets are all alike but every messy dataset is messy in its own way.”
&lt;footer&gt;
&lt;em&gt;— Hadley Wickham, Chief Scientist at RStudio&lt;/em&gt;
&lt;/footer&gt;
&lt;/blockquote&gt;
&lt;p&gt;This article will try to show how we can link the structure of a dataset to it’s meaning and make sure the data is showing what we want to show.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In my experience, a data analyst spends most of his or hers time on data cleaning and data validation. For quite a few companies, data cleaning means a series of manual tasks that take up a lot of time and require a lot of focus from the data analysts’ part. I am sure most of my colleagues can confirm this, especially those who work in an organisation that is slow to change its ways or work with people who are too accustomed to their ways that they are reluctant to adopt new tools and methods.&lt;/p&gt;
&lt;p&gt;Although as quoted at the beginning of this article, every dataset is messy in its own way, and common sense would suggest that the best way to clean it is to do it manually, I disagree. The are some ways in which one can automate the data cleaning process. Even if you do it partially is still a huge win.&lt;/p&gt;
&lt;p&gt;I do not agree with a manual approach to repetitive tasks for several reasons. Here are the ones that I consider the most important:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The quality of the work can vary between individuals. Some people are more organised than others and that can decrease the chances of overlooking something&lt;/li&gt;
&lt;li&gt;People get bored doing the same thing over and over again&lt;/li&gt;
&lt;li&gt;Doing tasks that take a lot of time and little creativity, data cleaning can be like that, can lower motivation and the analysts will no longer try to improve the datasets or find useful insights into the data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think it’s in everyone’s interest for the analysts to have free time to think of ways in which they can use the data to improve the company or the environment around them, not just make sure the data is there and it is correct, checking record by record. For me that used to be the most frustrating part of the job and it took me a lot of energy to find the motivation to start the work.&lt;/p&gt;
&lt;p&gt;In the above section I have discussed why data cleaning might be problematic when done manually. In the next section I would like to go into a bit more detail about what is data cleaning and how we can automate some of the processes done by a data analyst so they can focus on analysis more than data manipulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-data-cleaning-and-data-wrangling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is data cleaning and data wrangling&lt;/h1&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data cleaning&lt;/h2&gt;
&lt;p&gt;We will start with data cleaning as it is the more general term used by people and generally refers to a process through which the data quality is ensured. This can include but is not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ensuring the data format is correct (&lt;em&gt;e.g.&lt;/em&gt; the dates have a standard format in the data source so the analysis software can detect it)&lt;/li&gt;
&lt;li&gt;dealing with missing data&lt;/li&gt;
&lt;li&gt;dealing with outliers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When dealing with these cases, analysts don’t necessarily spend the time to analyse each observation individually, but they rather follow a predetermined set of rules, usually a general rule per variable. We can see some examples below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;changing the dates in a suitable format&lt;/li&gt;
&lt;li&gt;replacing missing records either with a &lt;em&gt;mean&lt;/em&gt;, &lt;em&gt;median&lt;/em&gt; or &lt;em&gt;remove&lt;/em&gt; them altogether&lt;/li&gt;
&lt;li&gt;eliminating outliers from the analysis or separating them in a different dataset so they can be studied in detail&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No matter which of these is encountered daily, there is no reason why this cannot be automated as we already have the logic for it, so we can program a computer to do the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data wrangling&lt;/h2&gt;
&lt;p&gt;Data wrangling is the process through which we manipulate the data so we can transform it to a format that is more suitable for our purposes. Data can come in a plethora of formats, however, when it comes to tabular data, which is the focus of this article, the format I found to be most useful and easiest to read and manipulate by most data software is a &lt;em&gt;Tidy Format&lt;/em&gt; quoted at the beginning of this article(see &lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We will see some interesting tools to use for data wrangling in the examples that follow.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning-in-practice&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data cleaning in practice&lt;/h1&gt;
&lt;p&gt;In the next section, we will discuss and go through a series of exercises that will allow us to clean and manipulate the data in a data source. For this we will use the &lt;code&gt;tidyverse&lt;/code&gt; package from &lt;em&gt;R&lt;/em&gt;. If you do not have it installed, you can do so by copying the following line into your console &lt;code&gt;install.packages(&#34;tidyverse&#34;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now for the actual analysis, we will start by activating it in the environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to play around with a dataset, we will use the &lt;em&gt;mtcars&lt;/em&gt; dataset that comes with the &lt;code&gt;tidyverse&lt;/code&gt; package. We can see the dataset using the code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%      # Our Dataset
      head()    # First six results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
#&amp;gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
#&amp;gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
#&amp;gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
#&amp;gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#&amp;gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
#&amp;gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are new to &lt;em&gt;R&lt;/em&gt; and are not familiar with the &lt;code&gt;%&amp;gt;%&lt;/code&gt; symbol used earlier, a good way to think of it is to consider that it links the actions attached to a dataset. It is to be read “take &lt;em&gt;dataset&lt;/em&gt; then (%&amp;gt;%)”. In this case it would be “take &lt;em&gt;mtcars&lt;/em&gt; then show the top 5 records”.&lt;/p&gt;
&lt;p&gt;Now that we have the dataset and the knowledge on how to read the code, let’s start with actual data cleaning and wrangling.&lt;/p&gt;
&lt;div id=&#34;mutate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mutate&lt;/h3&gt;
&lt;p&gt;As we can see, the row numbers in this data set are the car names. However, this is not very useful if we want to use the names in our analysis or group by name. So we will use a function that is very useful in data cleaning, &lt;code&gt;mutate()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mutate()&lt;/code&gt; allows the users to either create a new column at the end of a dataset, which is quite useful, or change an existing column. We will use this to create a new column with the names of the cars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  mutate(car_names = row.names(mtcars)) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
#&amp;gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
#&amp;gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
#&amp;gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
#&amp;gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#&amp;gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
#&amp;gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
#&amp;gt;                           car_names
#&amp;gt; Mazda RX4                 Mazda RX4
#&amp;gt; Mazda RX4 Wag         Mazda RX4 Wag
#&amp;gt; Datsun 710               Datsun 710
#&amp;gt; Hornet 4 Drive       Hornet 4 Drive
#&amp;gt; Hornet Sportabout Hornet Sportabout
#&amp;gt; Valiant                     Valiant&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There, we have added a new column to the dataset. Now, what if we need to filter something out of the data set?&lt;/p&gt;
&lt;p&gt;Luckily the &lt;code&gt;tidyverse&lt;/code&gt; is quite intuitive when it comes to naming functions and we can use the &lt;code&gt;filter()&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;filter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Filter&lt;/h3&gt;
&lt;p&gt;Let’s filter out from the dataset the cars with four cylinders (&lt;em&gt;cyl&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  filter(cyl != 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt;                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
#&amp;gt; Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
#&amp;gt; Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
#&amp;gt; Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
#&amp;gt; Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
#&amp;gt; Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
#&amp;gt; Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
#&amp;gt; Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
#&amp;gt; Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
#&amp;gt; Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
#&amp;gt; Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
#&amp;gt; Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
#&amp;gt; Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
#&amp;gt; Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
#&amp;gt; Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
#&amp;gt; Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
#&amp;gt; AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
#&amp;gt; Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
#&amp;gt; Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
#&amp;gt; Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
#&amp;gt; Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
#&amp;gt; Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see the &lt;em&gt;Datsun 710&lt;/em&gt; that had 4 cylinders has disappeared from the first 6 records.&lt;/p&gt;
&lt;p&gt;Of course, this function can also filter specific variables, and include multiple arguments. Let’s try to filter just the cars that have 4 cylinders and more that 90 horse power (&lt;em&gt;hp&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  filter(cyl == 4, hp &amp;gt; 90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt;                mpg cyl  disp  hp drat    wt  qsec vs am gear carb
#&amp;gt; Datsun 710    22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
#&amp;gt; Merc 230      22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
#&amp;gt; Toyota Corona 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
#&amp;gt; Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
#&amp;gt; Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
#&amp;gt; Volvo 142E    21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that this function will not work if you DO NOT use a double &lt;code&gt;=&lt;/code&gt; when trying to compare values. One &lt;code&gt;=&lt;/code&gt; means attribution in &lt;em&gt;R&lt;/em&gt;, which is why the function cannot accept it in a comparison.&lt;/p&gt;
&lt;p&gt;If you want to manipulate missing data in a column you can use the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filter(is.na(column_name))&lt;/code&gt; filters the rows that have a missing values for the specified column&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filter(!is.na(column_name))&lt;/code&gt; filters the rows that DO NOT have a missing values for the specified column&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Selecting columns&lt;/h3&gt;
&lt;p&gt;Maybe in some situations, you do not need to use the full dataset you have at your disposal.&lt;/p&gt;
&lt;p&gt;In this case, you can use a &lt;code&gt;select()&lt;/code&gt; statement that will allow you to work with just the needed dataset. Let’s select just two columns, &lt;em&gt;mpg&lt;/em&gt; and &lt;em&gt;cyl&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  select(mpg, cyl) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt;                    mpg cyl
#&amp;gt; Mazda RX4         21.0   6
#&amp;gt; Mazda RX4 Wag     21.0   6
#&amp;gt; Datsun 710        22.8   4
#&amp;gt; Hornet 4 Drive    21.4   6
#&amp;gt; Hornet Sportabout 18.7   8
#&amp;gt; Valiant           18.1   6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can select just the needed data without over exhausting our computing resources with unnecessary data.&lt;/p&gt;
&lt;p&gt;What if we need to use multiple datasets and it would be good to have them all in one place?&lt;/p&gt;
&lt;p&gt;For that we can use a &lt;code&gt;join()&lt;/code&gt; statement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Join&lt;/h3&gt;
&lt;p&gt;Joining two or more datasets is quite a straightforward process, however, I would like to take a moment and explain it a little bit for those who do not have experience with it and point out some possible aspects for which it would be good to watch out.&lt;/p&gt;
&lt;p&gt;The process of joining two datasets requires a reference column with common values in both tables. The joining process will look in the column from the first set, will search the values for each row in the other set and will bring all the values associated with that row in the first set. Let’s see a simple example below. We will create two dummy data sets so we can demonstrate this better.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters &amp;lt;- cbind(ID = c(1, 2, 3, 3, 1, 2),
                 Value1 = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;F&amp;quot;)) %&amp;gt;%
           as_tibble()

numbers &amp;lt;- cbind(ID = c(1, 2, 3, 4, 5, 6),
                 Value2 = c(&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;, &amp;quot;three&amp;quot;, &amp;quot;four&amp;quot;, &amp;quot;five&amp;quot;, &amp;quot;six&amp;quot;)) %&amp;gt;%
           as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 6 x 2
#&amp;gt;   ID    Value1
#&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
#&amp;gt; 1 1     A     
#&amp;gt; 2 2     B     
#&amp;gt; 3 3     C     
#&amp;gt; 4 3     D     
#&amp;gt; 5 1     E     
#&amp;gt; 6 2     F&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 6 x 2
#&amp;gt;   ID    Value2
#&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
#&amp;gt; 1 1     one   
#&amp;gt; 2 2     two   
#&amp;gt; 3 3     three 
#&amp;gt; 4 4     four  
#&amp;gt; 5 5     five  
#&amp;gt; 6 6     six&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the two datasets I would like to point out that, as you can see, the &lt;em&gt;ID&lt;/em&gt; values in the &lt;em&gt;numbers&lt;/em&gt; dataset are unique and each is associated with a different value. In the first set, they are not, each appearing twice. Now let’s check the join. We will use the &lt;code&gt;left_join()&lt;/code&gt; function as it is the most common.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters %&amp;gt;%
  left_join(numbers, by = &amp;quot;ID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 6 x 3
#&amp;gt;   ID    Value1 Value2
#&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; 
#&amp;gt; 1 1     A      one   
#&amp;gt; 2 2     B      two   
#&amp;gt; 3 3     C      three 
#&amp;gt; 4 3     D      three 
#&amp;gt; 5 1     E      one   
#&amp;gt; 6 2     F      two&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the values from &lt;em&gt;numbers&lt;/em&gt; associated with a particular &lt;em&gt;ID&lt;/em&gt; have been added to the &lt;em&gt;letters&lt;/em&gt; table. Note that the values higher than three are missing because they do not have an associated &lt;em&gt;ID&lt;/em&gt; in the first dataset. Now let’s see what happens if we try to do it the other way around. Remember that we need the joined table to have unique &lt;em&gt;ID&lt;/em&gt; values and here is why.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers %&amp;gt;%
  left_join(letters, by = &amp;quot;ID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 9 x 3
#&amp;gt;   ID    Value2 Value1
#&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; 
#&amp;gt; 1 1     one    A     
#&amp;gt; 2 1     one    E     
#&amp;gt; 3 2     two    B     
#&amp;gt; 4 2     two    F     
#&amp;gt; 5 3     three  C     
#&amp;gt; 6 3     three  D     
#&amp;gt; 7 4     four   &amp;lt;NA&amp;gt;  
#&amp;gt; 8 5     five   &amp;lt;NA&amp;gt;  
#&amp;gt; 9 6     six    &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please notice that the function created additional rows for each time it encountered the needed &lt;em&gt;ID&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pivoting-and-unpivoting-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pivoting and Unpivoting Data&lt;/h3&gt;
&lt;p&gt;Another useful function for a data analyst is to be able to pivot and un-pivot data. There are a couple of methods to do this. The simplest is using two functions &lt;code&gt;pivot_longer()&lt;/code&gt; and &lt;code&gt;pivot_wider()&lt;/code&gt;. The first function gathers multiple columns in one (makes a table longer) and the other function creates new columns using a previous column. The functionality is similar to that of a pivot_table in Excel.&lt;/p&gt;
&lt;p&gt;Let’s take a look at it in the following examples:&lt;/p&gt;
&lt;div id=&#34;pivot-wider&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Pivot wider&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  pivot_wider(names_from = cyl, values_from = mpg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 32 x 12
#&amp;gt;     disp    hp  drat    wt  qsec    vs    am  gear  carb   `6`   `4`   `8`
#&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt;  1  160    110  3.9   2.62  16.5     0     1     4     4  21    NA    NA  
#&amp;gt;  2  160    110  3.9   2.88  17.0     0     1     4     4  21    NA    NA  
#&amp;gt;  3  108     93  3.85  2.32  18.6     1     1     4     1  NA    22.8  NA  
#&amp;gt;  4  258    110  3.08  3.22  19.4     1     0     3     1  21.4  NA    NA  
#&amp;gt;  5  360    175  3.15  3.44  17.0     0     0     3     2  NA    NA    18.7
#&amp;gt;  6  225    105  2.76  3.46  20.2     1     0     3     1  18.1  NA    NA  
#&amp;gt;  7  360    245  3.21  3.57  15.8     0     0     3     4  NA    NA    14.3
#&amp;gt;  8  147.    62  3.69  3.19  20       1     0     4     2  NA    24.4  NA  
#&amp;gt;  9  141.    95  3.92  3.15  22.9     1     0     4     2  NA    22.8  NA  
#&amp;gt; 10  168.   123  3.92  3.44  18.3     1     0     4     4  19.2  NA    NA  
#&amp;gt; # ... with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function took the values from the &lt;strong&gt;cyl&lt;/strong&gt; column, created new columns with those values and filled them with the values from the &lt;strong&gt;mpg&lt;/strong&gt; column.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pivot-longer&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Pivot Longer&lt;/h4&gt;
&lt;p&gt;Pivot longer does the exact opposite. Let’s see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  pivot_wider(names_from = cyl, values_from = mpg) %&amp;gt;%
  pivot_longer(cols = c(&amp;quot;4&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;8&amp;quot;), names_to = &amp;quot;cyl&amp;quot;, values_to = &amp;quot;mpg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 96 x 11
#&amp;gt;     disp    hp  drat    wt  qsec    vs    am  gear  carb cyl     mpg
#&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt;  1   160   110  3.9   2.62  16.5     0     1     4     4 4      NA  
#&amp;gt;  2   160   110  3.9   2.62  16.5     0     1     4     4 6      21  
#&amp;gt;  3   160   110  3.9   2.62  16.5     0     1     4     4 8      NA  
#&amp;gt;  4   160   110  3.9   2.88  17.0     0     1     4     4 4      NA  
#&amp;gt;  5   160   110  3.9   2.88  17.0     0     1     4     4 6      21  
#&amp;gt;  6   160   110  3.9   2.88  17.0     0     1     4     4 8      NA  
#&amp;gt;  7   108    93  3.85  2.32  18.6     1     1     4     1 4      22.8
#&amp;gt;  8   108    93  3.85  2.32  18.6     1     1     4     1 6      NA  
#&amp;gt;  9   108    93  3.85  2.32  18.6     1     1     4     1 8      NA  
#&amp;gt; 10   258   110  3.08  3.22  19.4     1     0     3     1 4      NA  
#&amp;gt; # ... with 86 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now this created some extra rows, one for each extra column we previously created. We do not need three for each car considering that two of the are &lt;em&gt;NA’s&lt;/em&gt;. All we need to do in this situation is filter the data, eliminating the &lt;em&gt;NA&lt;/em&gt; values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  pivot_wider(names_from = cyl, values_from = mpg) %&amp;gt;%
  pivot_longer(cols = c(&amp;quot;4&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;8&amp;quot;), names_to = &amp;quot;cyl&amp;quot;, values_to = &amp;quot;mpg&amp;quot;) %&amp;gt;%
  filter(!is.na(mpg)) # notice the ! before the function in order to use the negative of the function&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 32 x 11
#&amp;gt;     disp    hp  drat    wt  qsec    vs    am  gear  carb cyl     mpg
#&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt;  1  160    110  3.9   2.62  16.5     0     1     4     4 6      21  
#&amp;gt;  2  160    110  3.9   2.88  17.0     0     1     4     4 6      21  
#&amp;gt;  3  108     93  3.85  2.32  18.6     1     1     4     1 4      22.8
#&amp;gt;  4  258    110  3.08  3.22  19.4     1     0     3     1 6      21.4
#&amp;gt;  5  360    175  3.15  3.44  17.0     0     0     3     2 8      18.7
#&amp;gt;  6  225    105  2.76  3.46  20.2     1     0     3     1 6      18.1
#&amp;gt;  7  360    245  3.21  3.57  15.8     0     0     3     4 8      14.3
#&amp;gt;  8  147.    62  3.69  3.19  20       1     0     4     2 4      24.4
#&amp;gt;  9  141.    95  3.92  3.15  22.9     1     0     4     2 4      22.8
#&amp;gt; 10  168.   123  3.92  3.44  18.3     1     0     4     4 6      19.2
#&amp;gt; # ... with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can use a select to rearrange the columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  pivot_wider(names_from = cyl, values_from = mpg) %&amp;gt;%
  pivot_longer(cols = c(&amp;quot;4&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;8&amp;quot;), names_to = &amp;quot;cyl&amp;quot;, values_to = &amp;quot;mpg&amp;quot;) %&amp;gt;%
  filter(!is.na(mpg)) %&amp;gt;%
  select(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 6 x 11
#&amp;gt;     mpg cyl    disp    hp  drat    wt  qsec    vs    am  gear  carb
#&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
#&amp;gt; 1  21   6       160   110  3.9   2.62  16.5     0     1     4     4
#&amp;gt; 2  21   6       160   110  3.9   2.88  17.0     0     1     4     4
#&amp;gt; 3  22.8 4       108    93  3.85  2.32  18.6     1     1     4     1
#&amp;gt; 4  21.4 6       258   110  3.08  3.22  19.4     1     0     3     1
#&amp;gt; 5  18.7 8       360   175  3.15  3.44  17.0     0     0     3     2
#&amp;gt; 6  18.1 6       225   105  2.76  3.46  20.2     1     0     3     1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
#&amp;gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
#&amp;gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
#&amp;gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
#&amp;gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#&amp;gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
#&amp;gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;As we could see in this article, &lt;em&gt;R&lt;/em&gt; is a tool that offers a lot of flexibility when it comes to data cleaning and data wrangling. Therefore I highly recommended that data analysts use this tool, or other automation tools, in their jobs as it will improve the data quality and it will reduce the time spent on frustrating or boring tasks.&lt;/p&gt;
&lt;p&gt;I know it can be a bit of a headache at first since the learning curve for &lt;em&gt;R&lt;/em&gt; can be a bit steep, however, I consider it is worth it. When I first started using &lt;em&gt;R&lt;/em&gt; I was a bit intimidated by the fact that I did not know where to start implementing it into my job. Then I realised that it is enough to automate part of it, not everything at once and the results started to show immediately. All I needed to do was think of what were the steps I had to take in order to check something or create a report, and recreate those steps in &lt;em&gt;R&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This is a very good first step. You will see that reports or tasks that can take hours or days can be done in a few seconds or minutes if you automate your work and let a script run.&lt;/p&gt;
&lt;p&gt;Now that you have a lot more free time, you have the time to let me know how it went or if you encountered any problems.&lt;/p&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
    <item>
      <title>Text Analysis with R</title>
      <link>https://www.vladaluas.com/post/text-analysis-with-r/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.vladaluas.com/post/text-analysis-with-r/</guid>
      <description>
        
          
&lt;script src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recently I found myself with some free time on my hands so I decided to learn a new skill, or at least start learning. So I thought to myself, what would be a good skill that would help me as a data analyst or would have helped me in the past? It had to be something that took a lot of time and that could be automated.&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Recently I found myself with some free time on my hands so I decided to learn a new skill, or at least start learning. So I thought to myself, what would be a good skill that would help me as a data analyst or would have helped me in the past? It had to be something that took a lot of time and that could be automated.&lt;/p&gt;
&lt;p&gt;And then it struck me, &lt;strong&gt;TEXT ANALYSIS!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I always hated the annoying task of having to analyse text, be it in the form of comments on the internet, transcripts from focus groups or something else, does not matter. As you might know, if you’ve done this sort of analysis, this can be a really boring and tedious work and can take a large amount of time. The way I used to do it was to read the comments, categorize each one then do some basic analysis based on those categories.&lt;/p&gt;
&lt;p&gt;You can avoid spending the huge amount of time on this situation if you learn how to analyse text using a programming language and here is why. You can be required to analyse text either as a recurring report or as a one time analysis. In both cases text analysis can be very beneficial.&lt;/p&gt;
&lt;p&gt;In the first case is kind of self-explanatory. You need to spend the time to set up the analysis, graphs and report, but this needs to be done just once and you can use it every time you want to refresh the report. The other solution is to analyse the data manually every time.&lt;/p&gt;
&lt;p&gt;Now, what about the second case, when you have a one time report? Wouldn’t it take just as much time to set up the report as it would to analyse it manually? Well, no and you will see in the article below how easy it can be to analyse data using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, in order to see how to analyse text using &lt;code&gt;R&lt;/code&gt; I have started reading &lt;a href=&#34;https://www.tidytextmining.com/index.html&#34;&gt;&lt;strong&gt;Text Mining with R&lt;/strong&gt;&lt;/a&gt; by &lt;em&gt;Julia Silge&lt;/em&gt; and &lt;em&gt;David Robinson&lt;/em&gt;. I highly recommend this book as their approach is to transform the text into a tidy format that allows you to easily analyse and visualize the results using graphs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disclaimers-and-the-structure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Disclaimers and the structure&lt;/h1&gt;
&lt;p&gt;I would like to shortly discuss the structure of the article and make some disclaimers about it, so we are on the same page.&lt;/p&gt;
&lt;p&gt;This article is intended just as an introductory example into what text analysis can do and how it can be used by data analysts, although I encourage you to study further if you think these methods can be useful. It is not intended as a comprehensive course on Natural Language Processing (NLP), as that is a complex topic that cannot be dealt with in just one article. Here, I will just show you three methods that can cover a great deal of analytical needs in a company.&lt;/p&gt;
&lt;p&gt;I would also like to point out that I will show some basic sentiment analysis methods, however they do not cover all the possibilities and are just the tip of the iceberg. That being said, with some tweaking, they can reliably be used as a starting point in the endeavours to automate this process, with more complex methods being added at a latter time.&lt;/p&gt;
&lt;p&gt;So, with the disclaimers out of the way we will discuss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Word Frequencies&lt;/li&gt;
&lt;li&gt;Comparisons Between Texts&lt;/li&gt;
&lt;li&gt;Sentiment Analysis&lt;/li&gt;
&lt;li&gt;Wordclouds&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;As a dataset, I though that a series of phone reviews would be a good starting point.&lt;/p&gt;
&lt;p&gt;As an analyst, it might be required of you to spend some time analysing reviews for different products your company makes and get insight from said reviews. As a dummy dataset I have chosen a series of reviews for the &lt;strong&gt;OnePlus&lt;/strong&gt; phone models. You can check the reviews &lt;a href=&#34;https://www.pcmag.com/categories/mobile-phones/brands/oneplus&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I could have chosen any other brand of phone, or any other product for that matter, however I own a &lt;strong&gt;OnePlus&lt;/strong&gt;. I want to check what is the general opinion about them and see if my decision was right or was it just bias.&lt;/p&gt;
&lt;p&gt;See, these text analysis skills can be used for selfish reasons as well, it doesn’t always have to be something &lt;em&gt;“useful”&lt;/em&gt; or &lt;em&gt;“productive”&lt;/em&gt;. You can learn them just so you can allow yourself to be too lazy to read a book or a review front to back.&lt;/p&gt;
&lt;p&gt;Now let’s download the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We will need a URL from where to download the data
# In this case we will do it from my GitHub repository
# You can download the data using this link
url &amp;lt;- &amp;quot;https://raw.github.com/VladAluas/Text_Analysis/master/Datasets/Text_review.csv&amp;quot;


# I prefer vroom to ingest csv, but you can use readr::read_csv() if you fancy it more
reviews &amp;lt;- vroom::vroom(url)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; Rows: 433
#&amp;gt; Columns: 3
#&amp;gt; Delimiter: &amp;quot;,&amp;quot;
#&amp;gt; chr [3]: Model, Segment, Text
#&amp;gt; 
#&amp;gt; Use `spec()` to retrieve the guessed column specification
#&amp;gt; Pass a specification to the `col_types` argument to quiet this message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;The structure of the data can be seen below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(reviews)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 6 x 3
#&amp;gt;   Model    Segment                Text                                          
#&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                                         
#&amp;gt; 1 OnePlus~ Introduction           &amp;quot;The days of the $600 smartphone aren&amp;#39;t over ~
#&amp;gt; 2 OnePlus~ Design, Features, and~ &amp;quot;The OnePlus One doesn&amp;#39;t feel like a sub-$400~
#&amp;gt; 3 OnePlus~ Design, Features, and~ &amp;quot;Our white test unit features a so-called sil~
#&amp;gt; 4 OnePlus~ Design, Features, and~ &amp;quot;The 5.5-inch, 1080p IPS display is on par wi~
#&amp;gt; 5 OnePlus~ Design, Features, and~ &amp;quot;There are two speaker grilles flanking the m~
#&amp;gt; 6 OnePlus~ Design, Features, and~ &amp;quot;With GSM (850/900/1800/1900MHz), UMTS (Bands~&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the data is structured in three columns: the model number, the segment of the review and the text from the segment.&lt;/p&gt;
&lt;p&gt;I have chosen to keep each paragraph from each review as a separate text because it’s easier to work with, and it’s more realistic. This is most likely how you might analyse the data when you read and compare the reviews section by section.&lt;/p&gt;
&lt;p&gt;Now that we have the data, I want to discuss the text analysis principles that I will use in this article.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;text-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Text Analysis&lt;/h1&gt;
&lt;p&gt;As you can see, it is quite hard to work with the data at the moment. You can’t count words or quantify them in any way, so we will need to transform the last column into a more analysis friendly format.&lt;/p&gt;
&lt;p&gt;As mentioned in the introduction, we will use some methods developed by &lt;em&gt;Julia Silge&lt;/em&gt; and &lt;em&gt;David Robinson&lt;/em&gt; in the &lt;code&gt;tidytext&lt;/code&gt; package. The function that we will &lt;em&gt;“abuse”&lt;/em&gt; in this article is &lt;code&gt;unnest_tokens()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This function allows us to transform the text column into a &lt;strong&gt;tidy&lt;/strong&gt; format (see &lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let’s see it in action.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We need to activate some additional libraries
# If you do not have the libraries installed you can install them using: install.packages(c(&amp;quot;tidyverse&amp;quot;, &amp;quot;tidytext&amp;quot;))
library(tidytext)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; Registered S3 methods overwritten by &amp;#39;readr&amp;#39;:
#&amp;gt;   method           from 
#&amp;gt;   format.col_spec  vroom
#&amp;gt;   print.col_spec   vroom
#&amp;gt;   print.collector  vroom
#&amp;gt;   print.date_names vroom
#&amp;gt;   print.locale     vroom
#&amp;gt;   str.col_spec     vroom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; -- Attaching packages --------------------------------------- tidyverse 1.3.0 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; v ggplot2 3.3.3     v purrr   0.3.4
#&amp;gt; v tibble  3.0.6     v dplyr   1.0.4
#&amp;gt; v tidyr   1.1.2     v stringr 1.4.0
#&amp;gt; v readr   1.4.0     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() --
#&amp;gt; x dplyr::filter() masks stats::filter()
#&amp;gt; x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews %&amp;gt;%
# We need to specify the name of the column to be created (Word) and the source column (Text)
  unnest_tokens(&amp;quot;Word&amp;quot;, &amp;quot;Text&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 30,067 x 3
#&amp;gt;    Model     Segment      Word      
#&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;     
#&amp;gt;  1 OnePlus 1 Introduction the       
#&amp;gt;  2 OnePlus 1 Introduction days      
#&amp;gt;  3 OnePlus 1 Introduction of        
#&amp;gt;  4 OnePlus 1 Introduction the       
#&amp;gt;  5 OnePlus 1 Introduction 600       
#&amp;gt;  6 OnePlus 1 Introduction smartphone
#&amp;gt;  7 OnePlus 1 Introduction aren&amp;#39;t    
#&amp;gt;  8 OnePlus 1 Introduction over      
#&amp;gt;  9 OnePlus 1 Introduction quite     
#&amp;gt; 10 OnePlus 1 Introduction yet       
#&amp;gt; # ... with 30,057 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the function took all the sentences from the &lt;strong&gt;Text&lt;/strong&gt; column and broke them down into a format that has one word per row and way more rows than before. So, our new data structure is one step away from a tidy format, all we need to do is count each word to see how many times it appears in the text, and then we will have a tidy format.&lt;/p&gt;
&lt;p&gt;I would also like to point out, as you have already probably noticed, that the function has transformed all the words to lower case and removed all the special symbols (&lt;em&gt;e.g.&lt;/em&gt; the $ from the price described in the introduction of the &lt;strong&gt;OnePlus 1&lt;/strong&gt;). This is important because it can save us a lot of headaches when cleaning the data.&lt;/p&gt;
&lt;p&gt;Now we will transform the data in a proper tidy format. To do so, we will &lt;code&gt;unnest&lt;/code&gt; the sentences, we will count each word, and then we can display the frequencies on a graph.&lt;/p&gt;
&lt;p&gt;Because we want to use the graph later, we will create a function, &lt;code&gt;word_frequency()&lt;/code&gt; that contains all the steps we want to apply to the graph. We will also replace some characters, so we will not double or under count some words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews_tidy &amp;lt;- reviews %&amp;gt;%
  unnest_tokens(&amp;quot;Word&amp;quot;, &amp;quot;Text&amp;quot;) %&amp;gt;%
# We also want to prevent the analysis in showing 6t and 6t&amp;#39;s as two separate words
  mutate(Word = str_replace(Word, &amp;quot;&amp;#39;s&amp;quot;, &amp;quot;&amp;quot;))
# We want to display graphically a word frequency plot
# We will create a function that will store all the operations we will repeat several times
word_frequency &amp;lt;- function(x, top = 10){
  
  x %&amp;gt;%
    
# We need a word count
  count(Word, sort = TRUE) %&amp;gt;%
  
# We want to create a factor from the word column with the levels showing the most frequent words as top level
# This is just for aestethic reasons, however, it helps make the point
  mutate(Word = factor(Word, levels = rev(unique(Word)))) %&amp;gt;% 
# We use the &amp;quot;top&amp;quot; variable defined in the function so we can decide how many words we want to use 
  top_n(top) %&amp;gt;%
    
# This will be useful later if we want to use a grouping variable and will do nothing if we don&amp;#39;t  
  ungroup() %&amp;gt;%
  
# The graph itself
  ggplot(mapping = aes(x = Word, y = n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL)
}

reviews_tidy %&amp;gt;%
  word_frequency(15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; Selecting by n&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/tidy_data_with_stop-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There, frequency analysis done. Now, what does the word &lt;strong&gt;the&lt;/strong&gt; say about the &lt;strong&gt;OnePlus&lt;/strong&gt; brand of phones?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nothing!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you can see and might have expected, determiners and conjunctions (&lt;em&gt;e.g.&lt;/em&gt; the, and, a, to) are the most frequently used words in any language and do not tell us much about the message of a sentence, not by themselves at least. These are called stop words, and we will eliminate them, so we can focus on the words that can give us a better picture of the text.&lt;/p&gt;
&lt;p&gt;Fortunately for us, the &lt;code&gt;tidytext&lt;/code&gt; package provides a dataset called &lt;code&gt;stop_words&lt;/code&gt; (what else) that contains a list of all the determiners and conjunctions, adverbs and adjectives that we can eliminate from a text, so we can analyse it properly.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This dataset contains only stop words from English. If you analyse a different language, you would have to use a different dataset or create one for yourself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With that in mind, I think we can recreate the previous graph after we eliminate the stop words and see what it tells us about the &lt;strong&gt;OnePlus&lt;/strong&gt; phones overall.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Same dataset as before with an extra code line
reviews_tidy &amp;lt;- reviews %&amp;gt;%
  unnest_tokens(&amp;quot;Word&amp;quot;, &amp;quot;Text&amp;quot;) %&amp;gt;%
  anti_join(stop_words, by = c(&amp;quot;Word&amp;quot; = &amp;quot;word&amp;quot;)) %&amp;gt;% # anti_join just keeps the rows common to both data sets
  mutate(Word = str_replace(Word, &amp;quot;&amp;#39;s&amp;quot;, &amp;quot;&amp;quot;))
# The graph is the same as before, we just changed the dataset
reviews_tidy %&amp;gt;%
  word_frequency(15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; Selecting by n&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/tidy_data_without_stop-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Slightly better, don’t you think?&lt;/p&gt;
&lt;p&gt;So, as an overall idea, we can see that the brand name (&lt;strong&gt;OnePlus&lt;/strong&gt;) is the most used, as we would expect. Then, we can see &lt;em&gt;phone&lt;/em&gt;, which is to be expected since we are talking about a product that is a phone.&lt;/p&gt;
&lt;p&gt;We can also see that &lt;em&gt;galaxy&lt;/em&gt; is mentioned quite a lot, just as much as &lt;em&gt;camera&lt;/em&gt; which is again expected. &lt;strong&gt;OnePlus&lt;/strong&gt; promoted themselves as a brand with high performance models at a cheaper price than a flagship from &lt;strong&gt;Samsung&lt;/strong&gt; or other makers, therefore it would be only natural to see the comparison between the two.&lt;/p&gt;
&lt;p&gt;However, if you reverse the analysis, you might not see &lt;strong&gt;OnePlus&lt;/strong&gt; in a &lt;strong&gt;Samsung&lt;/strong&gt; review because &lt;strong&gt;Samsung&lt;/strong&gt; is the &lt;strong&gt;gold standard&lt;/strong&gt; against which everyone is compared, while &lt;strong&gt;OnePlus&lt;/strong&gt; is not.&lt;/p&gt;
&lt;p&gt;Another pairing we see is &lt;em&gt;low&lt;/em&gt; and &lt;em&gt;light&lt;/em&gt; which is the part in the reviews where they are comparing camera performance in low light.&lt;/p&gt;
&lt;p&gt;Also you might have spotted that &lt;em&gt;7&lt;/em&gt; and &lt;em&gt;8&lt;/em&gt; are there as well. This is most likely because the &lt;em&gt;7&lt;/em&gt; from all the &lt;strong&gt;OnePlus 7&lt;/strong&gt; series is mentioned quite a lot, the same goes for the &lt;em&gt;8&lt;/em&gt;. This can be avoided, but it requires an extra step.&lt;/p&gt;
&lt;p&gt;You will need to replace the spaces in the model name (&lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;mutate(Word = str_replace(Word, &#34;OnePlus 7&#34;, &#34;OnePlus_7))&lt;/code&gt;), and do this for all the models not just &lt;strong&gt;OnePlus 7&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I will not do this, but you are welcome to try and let me know how the analysis changed.&lt;/p&gt;
&lt;p&gt;Now, the graph we used earlier shows us the most frequently used words across all texts in the corpus. This is useful because it gives us some good insight on what are the words most associated with &lt;strong&gt;OnePlus&lt;/strong&gt; as a brand overall.&lt;/p&gt;
&lt;p&gt;But, I would also like to have the top 5 words associated with each model. We can do so by adding two lines of code to the previous chunk. It’s as simple as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews_tidy %&amp;gt;%
  group_by(Model) %&amp;gt;% 
  word_frequency(5) +
  facet_wrap(~ Model, scales = &amp;quot;free_y&amp;quot;) # This is just to split the graph into multiple graphs for each model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; Selecting by n&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/word_frequency_facet_wrap-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cool, right?&lt;/p&gt;
&lt;p&gt;We have a matrix of graphics that shows us which terms are most frequently associated with a model and that is very useful from a business perspective. Although, keep in mind that these terms might require some cleaning. You might have words that are not useful for your analysis.&lt;/p&gt;
&lt;p&gt;There might be some cases in which you have a known fault in your product and it can be so frequent that it overshadows every other feedback. In this particular case an example can be the word &lt;strong&gt;oneplus&lt;/strong&gt;. This word has no relevance to the analysis, we know the name of the brand we are interested in, and having this word in the graphs might obscure the presence of a more relevant word.&lt;/p&gt;
&lt;p&gt;It can be anything else, let’s say a battery problem, you name it, I’m sure you have your own example in mind.&lt;/p&gt;
&lt;p&gt;Should that be the case, there is a simple solution for that. Add the undesired words to the &lt;code&gt;stop_words&lt;/code&gt; list and start the analysis all over again and it will give you the most frequent words that are of interest.&lt;/p&gt;
&lt;p&gt;I would suggest using this very sparsely, as you might overlook some crucial information that at some point seemed unimportant and now you forgot that is forcefully removed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-between-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparison between models&lt;/h1&gt;
&lt;p&gt;That is all well and good, but in the end, we still have to analyse the graphs using our instincts and determine what is the conclusion for each model, right? This cannot be automated!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yes, it can!!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For our purpose we will use a method called &lt;strong&gt;Term frequency - inverse document frequency&lt;/strong&gt; (&lt;strong&gt;tf_idf&lt;/strong&gt; for short and the fact that it rolls off the tongue easier). You can read more about this &lt;a href=&#34;https://www.tidytextmining.com/tfidf.html#zipfs-law&#34;&gt;here&lt;/a&gt;, but I will also give you a &lt;strong&gt;TL;DR&lt;/strong&gt; here.&lt;/p&gt;
&lt;p&gt;As you could see in the first graph, the most frequent terms in the review are the ones with no analytical value whatsoever, &lt;em&gt;the&lt;/em&gt;, &lt;em&gt;and&lt;/em&gt;, &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;etc&lt;/em&gt;. Words that have a high analytical value (&lt;em&gt;e.g.&lt;/em&gt; performance) will appear less often.&lt;/p&gt;
&lt;p&gt;Kepping that in mind, the &lt;strong&gt;tf_idf&lt;/strong&gt; method works based on this principle something like this. The word &lt;em&gt;oneplus&lt;/em&gt; is in all reviews so that does not tell us anything about a particular document. The model number on the other hand, is specific for each review and therefore way more important in helping us distinguish between the document. The same can go for the top tier rival phone models to &lt;strong&gt;OnePlus&lt;/strong&gt; at the time of the review (&lt;em&gt;e.g.&lt;/em&gt; &lt;strong&gt;Galaxy S20&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;This can be used to our advantage with a bit of a twist. We can check for the words that are frequent in one review and not the others to see what distinguishes one document from another.&lt;/p&gt;
&lt;p&gt;This comparison can be done with a simple formula &lt;code&gt;bind_tf_idf()&lt;/code&gt; that assigns weights to words using the principles below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;words with high frequency in all the documents: &lt;strong&gt;low weight&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;words with high frequency in just one of the documents and not the other: &lt;strong&gt;high weight&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;words with low frequency across the board: &lt;strong&gt;low weight&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s see this in practice:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;review_tf_idf &amp;lt;- 
  reviews_tidy %&amp;gt;%
    count(Model, Word, sort = TRUE) %&amp;gt;%
    bind_tf_idf(Word, Model, n)
review_tf_idf %&amp;gt;%
  arrange(desc(tf_idf))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 8,213 x 6
#&amp;gt;    Model              Word        n     tf   idf tf_idf
#&amp;gt;    &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
#&amp;gt;  1 OnePlus 6T         6t         31 0.0293 2.08  0.0609
#&amp;gt;  2 OnePlus 5T         5t         21 0.0195 2.77  0.0540
#&amp;gt;  3 OnePlus 6          s9         18 0.0183 2.77  0.0508
#&amp;gt;  4 OnePlus 7T McLaren mclaren    20 0.0292 1.67  0.0489
#&amp;gt;  5 OnePlus 2          s6         11 0.0164 2.77  0.0456
#&amp;gt;  6 OnePlus 7 Pro 5G   5g         40 0.0620 0.693 0.0430
#&amp;gt;  7 OnePlus 7T         7t         22 0.0306 1.39  0.0425
#&amp;gt;  8 OnePlus 8 Pro      s20        25 0.0245 1.67  0.0410
#&amp;gt;  9 OnePlus 8          s20        25 0.0223 1.67  0.0373
#&amp;gt; 10 OnePlus 3T         3t         21 0.0222 1.67  0.0372
#&amp;gt; # ... with 8,203 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can display this using plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;review_tf_idf %&amp;gt;%
  
# We need to sort the data in descending order so we can create the factors for each term
  arrange(desc(tf_idf)) %&amp;gt;%
# We create the factors as we did previously
  mutate(Word = factor(Word, levels = rev(unique(Word)))) %&amp;gt;%
# Select just the top 5 words for each model
  group_by(Model) %&amp;gt;%
  top_n(5) %&amp;gt;%
  ungroup() %&amp;gt;%
# Our Plot
  ggplot(mapping = aes(x = Word, y = tf_idf, fill = Model)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL) +
  coord_flip() +
  facet_wrap(~ Model, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; Selecting by tf_idf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/tf_idf_graph-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, these are the main items that separate one review from the other. Amongst them we can see the main flagship of &lt;strong&gt;Samsung&lt;/strong&gt;, especially for latter reviews, they seem to compare the brands quite a lot.&lt;/p&gt;
&lt;p&gt;We can also single out that for &lt;strong&gt;One Plus 7 Pro 5G&lt;/strong&gt; there is a problem with overheating and the &lt;strong&gt;OnePlus 6&lt;/strong&gt; is described as elegant.&lt;/p&gt;
&lt;p&gt;Of course, this can be tweaked quite a bit depending on your needs. You can eliminate words, you can replace some of them, or you can add a different grouping to the analysis. I’m pretty sure you have an idea on what you would change when you apply it to your analytic needs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sentiment Analysis&lt;/h1&gt;
&lt;p&gt;Now, finally the good part.&lt;/p&gt;
&lt;p&gt;In this segment, I would like to discuss some basic principles of sentiment analysis and how they can be used in data analysis to quickly get an idea about your product. So, how do we achieve that?&lt;/p&gt;
&lt;p&gt;Well, the most basic method, and the one that we will cover today, is to simply associate each word in the review to a sentiment. Then it becomes a simple matter of counting how many words are associated with positive or negative sentiments to get the overall affect of the text.&lt;/p&gt;
&lt;p&gt;This is quite straight forward and the &lt;code&gt;tidyverse&lt;/code&gt; package comes to our aid with some libraries that already have these associations made and are also validated against multiple sources. The libraries are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AFFIN&lt;/code&gt; from &lt;a href=&#34;http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010&#34;&gt;Finn Årup Nielsen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bing&lt;/code&gt; from &lt;a href=&#34;https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html&#34;&gt;Bing Liu and collaborators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nrc&lt;/code&gt; from &lt;a href=&#34;http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm&#34;&gt;Saif Mohammad and Peter Turney&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these libraries is helpful in its own way and approaches sentiment analysis differently. Let’s check them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;afinn&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 2,477 x 2
#&amp;gt;    word       value
#&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
#&amp;gt;  1 abandon       -2
#&amp;gt;  2 abandoned     -2
#&amp;gt;  3 abandons      -2
#&amp;gt;  4 abducted      -2
#&amp;gt;  5 abduction     -2
#&amp;gt;  6 abductions    -2
#&amp;gt;  7 abhor         -3
#&amp;gt;  8 abhorred      -3
#&amp;gt;  9 abhorrent     -3
#&amp;gt; 10 abhors        -3
#&amp;gt; # ... with 2,467 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;bing&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 6,786 x 2
#&amp;gt;    word        sentiment
#&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;    
#&amp;gt;  1 2-faces     negative 
#&amp;gt;  2 abnormal    negative 
#&amp;gt;  3 abolish     negative 
#&amp;gt;  4 abominable  negative 
#&amp;gt;  5 abominably  negative 
#&amp;gt;  6 abominate   negative 
#&amp;gt;  7 abomination negative 
#&amp;gt;  8 abort       negative 
#&amp;gt;  9 aborted     negative 
#&amp;gt; 10 aborts      negative 
#&amp;gt; # ... with 6,776 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;nrc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 13,901 x 2
#&amp;gt;    word        sentiment
#&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;    
#&amp;gt;  1 abacus      trust    
#&amp;gt;  2 abandon     fear     
#&amp;gt;  3 abandon     negative 
#&amp;gt;  4 abandon     sadness  
#&amp;gt;  5 abandoned   anger    
#&amp;gt;  6 abandoned   fear     
#&amp;gt;  7 abandoned   negative 
#&amp;gt;  8 abandoned   sadness  
#&amp;gt;  9 abandonment anger    
#&amp;gt; 10 abandonment fear     
#&amp;gt; # ... with 13,891 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I want to discuss these libraries just a bit.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;AFINN&lt;/code&gt; library gives a score between -5 and +5 to each word. Once this is done, the sentiment can be inferred by summing up the scores.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;bing&lt;/code&gt; library simply associates a word with a negative or positive valence. At the end we can count how many words are positive or negative.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;nrc&lt;/code&gt; library is interesting because it gives you a list of words that can be classified in multiple ways. As you can see, the second element, can be classified either as &lt;strong&gt;fear&lt;/strong&gt;, &lt;strong&gt;negative&lt;/strong&gt; or &lt;strong&gt;sadness&lt;/strong&gt;. This is useful if you want to check for a specific sentiment, or a list of specific sentiments in a text (&lt;em&gt;e.g.&lt;/em&gt; just how many terms are associated with fear)&lt;/p&gt;
&lt;p&gt;Let’s proceed by using the &lt;code&gt;AFINN&lt;/code&gt; library to check the sentiment for each model and see how they perform. We will use just the conclusion for each review as that should be the most relevant in transmitting the overall sentiment for the whole review.&lt;/p&gt;
&lt;p&gt;However, we have to keep in mind that these being technical reviews, they might contain a terminology different from the one used in natural language, and the analysis might not be as accurate as an analysis on Facebook posts, for example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conclusion_afinn &amp;lt;- reviews %&amp;gt;%
  filter(str_detect(Segment, &amp;quot;Conclusion&amp;quot;)) %&amp;gt;%
  unnest_tokens(&amp;quot;Word&amp;quot;, &amp;quot;Text&amp;quot;) %&amp;gt;%
  anti_join(stop_words, by = c(&amp;quot;Word&amp;quot; = &amp;quot;word&amp;quot;)) %&amp;gt;%
# We will get the sentiments with a inner_join since the words that don&amp;#39;t have a match, don&amp;#39;t have a score value
  inner_join(get_sentiments(&amp;quot;afinn&amp;quot;), by = c(&amp;quot;Word&amp;quot; = &amp;quot;word&amp;quot;))

conclusion_afinn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 122 x 4
#&amp;gt;    Model     Segment                 Word     value
#&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
#&amp;gt;  1 OnePlus 1 Cameras and Conclusions cut         -1
#&amp;gt;  2 OnePlus 1 Cameras and Conclusions true         2
#&amp;gt;  3 OnePlus 1 Cameras and Conclusions alive        1
#&amp;gt;  4 OnePlus 1 Cameras and Conclusions true         2
#&amp;gt;  5 OnePlus 1 Cameras and Conclusions miss        -2
#&amp;gt;  6 OnePlus 1 Cameras and Conclusions straight     1
#&amp;gt;  7 OnePlus 1 Cameras and Conclusions capable      1
#&amp;gt;  8 OnePlus 1 Cameras and Conclusions free         1
#&amp;gt;  9 OnePlus 1 Cameras and Conclusions demand      -1
#&amp;gt; 10 OnePlus 1 Cameras and Conclusions impress      3
#&amp;gt; # ... with 112 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, each token has been unnested, and assigned a sentiment value.&lt;/p&gt;
&lt;p&gt;Now, in order to check the sentiments for each review, all we need to do is add the scores and plot them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conclusion_afinn %&amp;gt;%
  group_by(Model) %&amp;gt;%
  summarise(Score = sum(value)) %&amp;gt;%
  arrange(desc(Score)) %&amp;gt;%
  mutate(Model = factor(Model, levels = rev(unique(Model)))) %&amp;gt;%
  ggplot(mapping = aes(x = Model, y = Score)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/conclusions_affin_graph-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The scores are in and overall the &lt;strong&gt;Oneplus 2&lt;/strong&gt; has the best reviews.&lt;/p&gt;
&lt;p&gt;However, what if we want to see a report on which model has the most positive and negative reviews? For that we would use the &lt;code&gt;bing&lt;/code&gt; library.&lt;/p&gt;
&lt;p&gt;Let’s see how:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conclusion_bing &amp;lt;- reviews %&amp;gt;%
  filter(str_detect(Segment, &amp;quot;Conclusion&amp;quot;)) %&amp;gt;%
  unnest_tokens(&amp;quot;Word&amp;quot;, &amp;quot;Text&amp;quot;) %&amp;gt;%
  anti_join(stop_words, by = c(&amp;quot;Word&amp;quot; = &amp;quot;word&amp;quot;)) %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;bing&amp;quot;), by = c(&amp;quot;Word&amp;quot; = &amp;quot;word&amp;quot;))

conclusion_bing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 189 x 4
#&amp;gt;    Model     Segment                 Word       sentiment
#&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;    
#&amp;gt;  1 OnePlus 1 Cameras and Conclusions led        positive 
#&amp;gt;  2 OnePlus 1 Cameras and Conclusions distortion negative 
#&amp;gt;  3 OnePlus 1 Cameras and Conclusions miss       negative 
#&amp;gt;  4 OnePlus 1 Cameras and Conclusions dynamic    positive 
#&amp;gt;  5 OnePlus 1 Cameras and Conclusions distortion negative 
#&amp;gt;  6 OnePlus 1 Cameras and Conclusions warped     negative 
#&amp;gt;  7 OnePlus 1 Cameras and Conclusions unnatural  negative 
#&amp;gt;  8 OnePlus 1 Cameras and Conclusions admirable  positive 
#&amp;gt;  9 OnePlus 1 Cameras and Conclusions soft       positive 
#&amp;gt; 10 OnePlus 1 Cameras and Conclusions prefer     positive 
#&amp;gt; # ... with 179 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can proceed with the same steps, just add the sentiment to the grouping.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conclusion_bing %&amp;gt;%
  group_by(Model, sentiment) %&amp;gt;%
  count() %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(Model = reorder(Model, n)) %&amp;gt;%
  ggplot(mapping = aes(x = Model, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, y = &amp;quot;Negative vs positive sentiment / Model&amp;quot;) +
  facet_wrap(~ sentiment, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/conclusions_bing_graph-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can see a more nuanced approach.&lt;/p&gt;
&lt;p&gt;For example, the &lt;strong&gt;OnePlus 6T&lt;/strong&gt; and the &lt;strong&gt;OnePlus 7 (for China)&lt;/strong&gt; have no negative reviews, but they also have only a few positive things said about them. This seems to be reflected in their placement in the previous graph as well.&lt;/p&gt;
&lt;p&gt;A curious case can be for the &lt;strong&gt;OnePlus 3&lt;/strong&gt; which seems to have more positive than negative reviews, however as an overall score it is dead last on a positivity ranking. This indicates that the review did not regard this model with high praise or the really negative descriptions were very negative. Most likely a combination of both.&lt;/p&gt;
&lt;p&gt;Both these approaches have their advantages and disadvantages and in practice you will most likely use a combination of both, not just one. It is really useful to view a problem from multiple angles. You never know which method is helpful for the decision makers in your company.&lt;/p&gt;
&lt;p&gt;With that in mind, I would like to discuss one more method of presenting the data that might help in some situations more than graphs.&lt;/p&gt;
&lt;div id=&#34;wordclouds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;WordClouds&lt;/h2&gt;
&lt;p&gt;In this section I would like to show you a different approach in presenting the data, &lt;strong&gt;WordClouds&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I personally find them very useful when you are trying to communicate the prevalence of a word in a text or speech. They basically have the same role as a pie chart, but they’re way better because they display data in a more user-friendly way. Using a wordcloud will allow you to look at it and see how frequent a word is without having to check and re-check a legend for dozens of times.&lt;/p&gt;
&lt;p&gt;With that said, let’s check our wordcloud. It should show the same data as the first graph, just in a different display style, so I will use the same data set &lt;code&gt;reviews_tidy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this I will use the &lt;code&gt;wordcloud&lt;/code&gt; package. If you do not have it installed, you can install it by using &lt;code&gt;install.packages(&#34;wordcloud&#34;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)
#&amp;gt; Loading required package: RColorBrewer

reviews_tidy %&amp;gt;%
  count(Word) %&amp;gt;%
  with(wordcloud(Word, n, max.words = 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/wordcloud_overall-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the results are similar to the first analysis, the more frequent a word, the larger the font. However, with this type of graph we can include a lot more items. In this we have included 100 words, as opposed to 15 in the first graph.&lt;/p&gt;
&lt;p&gt;Now, this clearly shows that the most prevalent word is &lt;strong&gt;oneplus&lt;/strong&gt; followed by &lt;strong&gt;phone&lt;/strong&gt; then &lt;strong&gt;pro&lt;/strong&gt; and so on, exactly the same information we had before, the only difference is that we have a bigger picture using this method.&lt;/p&gt;
&lt;p&gt;As mentioned, it’s a very useful way to show the prevalence of multiple words in a text.&lt;/p&gt;
&lt;p&gt;Now, I would like to show you how you can use a wordcloud for sentiment analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;reshape2&amp;#39;
#&amp;gt; The following object is masked from &amp;#39;package:tidyr&amp;#39;:
#&amp;gt; 
#&amp;gt;     smiths


reviews_tidy %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;bing&amp;quot;), by = c(&amp;quot;Word&amp;quot; = &amp;quot;word&amp;quot;)) %&amp;gt;%
  count(Word, sentiment, sort = TRUE) %&amp;gt;%
  acast(Word ~ sentiment, value.var = &amp;quot;n&amp;quot;, fill = 0) %&amp;gt;%
  comparison.cloud(colors = c(&amp;quot;#202121&amp;quot;, &amp;quot;#797C80&amp;quot;),
                   max.words = 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.vladaluas.com/post/text-analysis-with-r/index.en_files/figure-html/wordcloud_sentiment-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a very quick and useful way to show which elements influence the sentiment for your product the most and make decisions based on it.&lt;/p&gt;
&lt;p&gt;We can clearly see that the words that influence the most the negative scores are &lt;em&gt;noise&lt;/em&gt;, &lt;em&gt;expensive&lt;/em&gt; and &lt;em&gt;loud&lt;/em&gt; while the ones that influence the positive reviews are &lt;em&gt;excellent&lt;/em&gt;, &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;smooth&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis-wrap-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment Analysis wrap up&lt;/h2&gt;
&lt;p&gt;There is more to be said about sentiment analysis, however in this article I just wanted to give you a short introduction and show some basic principles for it.&lt;/p&gt;
&lt;p&gt;I’m sure you have noticed some of the quirks yourself. For example the lexicons we have used here are intended for just one word, and that can miss the sentiment of a phase (&lt;em&gt;e.g.&lt;/em&gt; &lt;em&gt;not good&lt;/em&gt; is a negative term, however the lexicon will see &lt;em&gt;not&lt;/em&gt; as neutral and &lt;em&gt;good&lt;/em&gt; as positive, therefore overall it will see it as positive). In order to avoid situations like this we can use pairing of words and check for these types of situation.&lt;/p&gt;
&lt;p&gt;I plan to go into more detail in a series of articles on the subject, so if you find something interesting or have a topic to discuss, please let me know.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;When I read the book, I realised that this type of analysis is very scalable, if done with a programming language like &lt;code&gt;R&lt;/code&gt;. By this I mean that a computer will have no problem in analysing 10 or 30.000 words in about the same time. For a human the difference is huge.&lt;/p&gt;
&lt;p&gt;Besides being scalable, this analysis can be done multiple times without having to change the code or spend the time setting everything up.&lt;/p&gt;
&lt;p&gt;The last advantage of this is that this method is not prone to human error or fatigue. Let’s be honest, we as humans get tired after some time spent in the same task, and we can get errors. Why not avoid this if possible?&lt;/p&gt;
&lt;p&gt;If you have to do these types of analyses, please let me know if you think this article could be useful in your day to day work, and if you have applied them, please let me know how it influenced your time spent on the analysis.&lt;/p&gt;
&lt;p&gt;Also, check &lt;em&gt;Julia&lt;/em&gt; and &lt;em&gt;David’s&lt;/em&gt; &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;book&lt;/a&gt; and if it helped you and you can afford it, buy it to show your support of their great work.&lt;/p&gt;
&lt;/div&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
