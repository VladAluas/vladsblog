---
title: Data Cleaning
author: Vlad Aluas
date: '`r Sys.Date()`'
slug: data-cleaning
categories: ["Tutorial"]
tags: ["R", "tidyverse", "data cleaning"]
thumbnail: /images/code-logo.png
toc: true
---

</br>

> "Like families, tidy datasets are all alike but every messy dataset is messy in its own way." `r tufte::quote_footer('*--- Hadley Wickham, Chief Scientist at RStudio*')`

This article will try to show how we can link the structure of a dataset to it's meaning and make sure the data is showing what we want to show.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", collapse = FALSE)
```

------------------------------------------------------------------------

# Introduction

In my experience, a data analyst spends most of his or hers time on data cleaning and data validation. For quite a few companies, data cleaning means a series of manual tasks that take up a lot of time and require a lot of focus from the data analysts' part. I am sure most of my colleagues can confirm this, especially those who work in an organisation that is slow to change its ways or work with people who are too accustomed to their ways that they are reluctant to adopt new tools and methods.

Although as quoted at the beginning of this article, every dataset is messy in its own way, and common sense would suggest that the best way to clean it is to do it manually, I disagree. The are some ways in which one can automate the data cleaning process. Even if you do it partially is still a huge win.

I do not agree with a manual approach to repetitive tasks for several reasons. Here are the ones that I consider the most important:

-   The quality of the work can vary between individuals. Some people are more organised than others and that can decrease the chances of overlooking something
-   People get bored doing the same thing over and over again
-   Doing tasks that take a lot of time and little creativity, data cleaning can be like that, can lower motivation and the analysts will no longer try to improve the datasets or find useful insights into the data

I think it's in everyone's interest for the analysts to have free time to think of ways in which they can use the data to improve the company or the environment around them, not just make sure the data is there and it is correct, checking record by record. For me that used to be the most frustrating part of the job and it took me a lot of energy to find the motivation to start the work.

In the above section I have discussed why data cleaning might be problematic when done manually. In the next section I would like to go into a bit more detail about what is data cleaning and how we can automate some of the processes done by a data analyst so they can focus on analysis more than data manipulation.

# What is data cleaning and data wrangling

## Data cleaning

We will start with data cleaning as it is the more general term used by people and generally refers to a process through which the data quality is ensured. This can include but is not limited to:

-   ensuring the data format is correct (*e.g.* the dates have a standard format in the data source so the analysis software can detect it)
-   dealing with missing data
-   dealing with outliers

When dealing with these cases, analysts don't necessarily spend the time to analyse each observation individually, but they rather follow a predetermined set of rules, usually a general rule per variable. We can see some examples below:

-   changing the dates in a suitable format
-   replacing missing records either with a *mean*, *median* or *remove* them altogether
-   eliminating outliers from the analysis or separating them in a different dataset so they can be studied in detail

No matter which of these is encountered daily, there is no reason why this cannot be automated as we already have the logic for it, so we can program a computer to do the same.

## Data wrangling

Data wrangling is the process through which we manipulate the data so we can transform it to a format that is more suitable for our purposes. Data can come in a plethora of formats, however, when it comes to tabular data, which is the focus of this article, the format I found to be most useful and easiest to read and manipulate by most data software is a *Tidy Format* quoted at the beginning of this article(see [here](https://vita.had.co.nz/papers/tidy-data.pdf)).

We will see some interesting tools to use for data wrangling in the examples that follow.

# Data cleaning in practice

In the next section, we will discuss and go through a series of exercises that will allow us to clean and manipulate the data in a data source. For this we will use the `tidyverse` package from *R*. If you do not have it installed, you can do so by copying the following line into your console `install.packages("tidyverse")`.

Now for the actual analysis, we will start by activating it in the environment.

```{r libraries, message=FALSE}
library(tidyverse)
```


In order to play around with a dataset, we will use the *mtcars* dataset that comes with the `tidyverse` package. We can see the dataset using the code below:

```{r mtcars_head}
mtcars %>%      # Our Dataset
      head()    # First six results
```

If you are new to *R* and are not familiar with the `%>%` symbol used earlier, a good way to think of it is to consider that it links the actions attached to a dataset. It is to be read "take *dataset* then (%\>%)". In this case it would be "take *mtcars* then show the top 5 records".

Now that we have the dataset and the knowledge on how to read the code, let's start with actual data cleaning and wrangling.

### Mutate

As we can see, the row numbers in this data set are the car names. However, this is not very useful if we want to use the names in our analysis or group by name. So we will use a function that is very useful in data cleaning, `mutate()`. 

`mutate()` allows the users to either create a new column at the end of a dataset, which is quite useful, or change an existing column. We will use this to create a new column with the names of the cars.

```{r mutate_example}
mtcars %>%
  mutate(car_names = row.names(mtcars)) %>%
  head()
```

There, we have added a new column to the dataset. Now, what if we need to filter something out of the data set? 

Luckily the `tidyverse` is quite intuitive when it comes to naming functions and we can use the `filter()` function.

### Filter

Let's filter out from the dataset the cars with four cylinders (*cyl*).

```{r filter_cyl}
mtcars %>%
  filter(cyl != 4)
```

As we can see the *Datsun 710* that had 4 cylinders has disappeared from the first 6 records.

Of course, this function can also filter specific variables, and include multiple arguments. Let's try to filter just the cars that have 4 cylinders and more that 90 horse power (*hp*).

```{r filter_cyl_hp}
mtcars %>%
  filter(cyl == 4, hp > 90)
```

Please note that this function will not work if you DO NOT use a double `=` when trying to compare values. One `=` means attribution in *R*, which is why the function cannot accept it in a comparison.

If you want to manipulate missing data in a column you can use the following:

*   `filter(is.na(column_name))` filters the rows that have a missing values for the specified column 
*   `filter(!is.na(column_name))` filters the rows that DO NOT have a missing values for the specified column 

### Selecting columns

Maybe in some situations, you do not need to use the full dataset you have at your disposal. 

In this case, you can use a `select()` statement that will allow you to work with just the needed dataset. Let's select just two columns, *mpg* and *cyl*.

```{r select_example}
mtcars %>%
  select(mpg, cyl) %>%
  head()
```

Now we can select just the needed data without over exhausting our computing resources with unnecessary data.

What if we need to use multiple datasets and it would be good to have them all in one place? 

For that we can use a `join()` statement.

### Join

Joining two or more datasets is quite a straightforward process, however, I would like to take a moment and explain it a little bit for those who do not have experience with it and point out some possible aspects for which it would be good to watch out.

The process of joining two datasets requires a reference column with common values in both tables. The joining process will look in the column from the first set, will search the values for each row in the other set and will bring all the values associated with that row in the first set. Let's see a simple example below. We will create two dummy data sets so we can demonstrate this better.

```{r join_datasets}
letters <- cbind(ID = c(1, 2, 3, 3, 1, 2),
                 Value1 = c("A", "B", "C", "D", "E", "F")) %>%
           as_tibble()

numbers <- cbind(ID = c(1, 2, 3, 4, 5, 6),
                 Value2 = c("one", "two", "three", "four", "five", "six")) %>%
           as_tibble()
```

```{r join_letters}
letters
```

````{r join_numbers}
numbers
```

Now that we have the two datasets I would like to point out that, as you can see, the *ID* values in the *numbers* dataset are unique and each is associated with a different value. In the first set, they are not, each appearing twice. Now let's check the join. We will use the `left_join()` function as it is the most common.

```{r join_example_1}
letters %>%
  left_join(numbers, by = "ID")
```

As we can see, the values from *numbers* associated with a particular *ID* have been added to the *letters* table. Note that the values higher than three are missing because they do not have an associated *ID* in the first dataset. Now let's see what happens if we try to do it the other way around. Remember that we need the joined table to have unique *ID* values and here is why.

```{r join_example_2}
numbers %>%
  left_join(letters, by = "ID")
```

Please notice that the function created additional rows for each time it encountered the needed *ID*.

### Pivoting and Unpivoting Data

Another useful function for a data analyst is to be able to pivot and un-pivot data. There are a couple of methods to do this. The simplest is using two functions `pivot_longer()` and `pivot_wider()`. The first function gathers multiple columns in one (makes a table longer) and the other function creates new columns using a previous column. The functionality is similar to that of a pivot_table in Excel.

Let's take a look at it in the following examples:

#### Pivot wider

```{r pivot_wider}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg)
```

The function took the values from the **cyl** column, created new columns with those values and filled them with the values from the **mpg** column.

#### Pivot Longer

Pivot longer does the exact opposite. Let's see.

```{r pivot_longer}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg) %>%
  pivot_longer(cols = c("4", "6", "8"), names_to = "cyl", values_to = "mpg")

```

Now this created some extra rows, one for each extra column we previously created. We do not need three for each car considering that two of the are *NA's*. All we need to do in this situation is filter the data, eliminating the *NA* values.

```{r pivot_longer_filter}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg) %>%
  pivot_longer(cols = c("4", "6", "8"), names_to = "cyl", values_to = "mpg") %>%
  filter(!is.na(mpg)) # notice the ! before the function in order to use the negative of the function

```

And now we can use a select to rearrange the columns.

```{r multi_pivoted_mtcars}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg) %>%
  pivot_longer(cols = c("4", "6", "8"), names_to = "cyl", values_to = "mpg") %>%
  filter(!is.na(mpg)) %>%
  select(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) %>%
  head()
```

```{r initial_data}
mtcars %>%
  head()
```

# Conclusions

As we could see in this article, *R* is a tool that offers a lot of flexibility when it comes to data cleaning and data wrangling. Therefore I highly recommended that data analysts use this tool, or other automation tools, in their jobs as it will improve the data quality and it will reduce the time spent on frustrating or boring tasks.

I know it can be a bit of a headache at first since the learning curve for *R* can be a bit steep, however, I consider it is worth it. When I first started using *R* I was a bit intimidated by the fact that I did not know where to start implementing it into my job. Then I realised that it is enough to automate part of it, not everything at once and the results started to show immediately. All I needed to do was think of what were the steps I had to take in order to check something or create a report, and recreate those steps in *R*.

This is a very good first step. You will see that reports or tasks that can take hours or days can be done in a few seconds or minutes if you automate your work and let a script run.

Now that you have a lot more free time, you have the time to let me know how it went or if you encountered any problems.
uation o bit, however the solution is to include in the `plt` object just the desired code, and everything else that is changing is left out of the object. In this case, we will leave the `ggplot()` function out as well.

```{r dry_code_df}
plt <- mtcars %>%
          mutate(car_names = rownames(mtcars),
                 cyl = as.factor(cyl))
# The code becomes
plt %>%
  ggplot(mapping = aes(x = cyl, y = mpg)) +
  geom_boxplot() +
  geom_point() +
  geom_text(mapping = aes(label = car_names), hjust = 0, nudge_x = 0.05, check_overlap = TRUE)
```

As we can see this can help us reduce the number of lines of code. In this particular case not by much, however, if you have to type the 10 lines of code for one dataset transformation over and over again, it is very useful to store all those transformations is an object. In this particular case, that object would be another dataset.

Although this particular example is a basic one, we just created a new dataset that is easier to work with, the true power of *R* being object oriented, and what helps the most in keeping your code **DRY** is what we will discuss in the next section, *custom functions*.


#### Custom Functions

Custom functions are another way to keep your code **DRY**. They are a powerful tool that can assist you keep your code easily maintainable and with fewer line of code. 

They allow you to store and standardise the steps you would normally apply a large range of objects, datasets, columns or even other functions.

In order to understand this principle better, we will create a dataset from scratch to serve as a clear example of the usefulness of custom functions. You can create it as well using the code below:

```{r random_data_frame}
# We will set a seed for the random generator so we can get the same data should you chose to use the same code
set.seed(123)
# We need to generate some random data, so I will use the rnorm() function. You can use other functions if you like
df <-     # our data frame
  cbind(  # It's easier to generate columns and bind them together
    tibble(x = rnorm(10, 0, 2)),
    tibble(y = rnorm(10, 4, 9)),
    tibble(z = rnorm(10, 15, 1)),
    tibble(q = rnorm(10, 2, 18))
  )
# Now we need some NA values. We will add them at the end
df <- 
  rbind( # we will simply bind some rows at the end
  df,
  c(NA, NA, NA, NA),
  c(NA, NA, NA, NA),
  c(NA, NA, NA, NA),
  c(NA, NA, NA, NA),
  c(NA, NA, NA, NA))
```

Now that this dataset has some missing data, let's say that you want to replace the missing data with some values. How do you do this? The most used practises just replace the missing values with the *mean* value or the *median* value. 

Let's say you decide to use the *mean* value. In practice, you would do the following:

```{r repeated_mutate_mean}
df %>%
  mutate(x = ifelse(is.na(x), mean(x, na.rm = T), x),
         y = ifelse(is.na(y), mean(y, na.rm = T), y),
         z = ifelse(is.na(z), mean(z, na.rm = T), z),
         q = ifelse(is.na(q), mean(q, na.rm = T), q))
```

Remember what we mentioned earlier, that is a good idea to not repeat your code more than 3-4 times? In this example is all good since we only have four lines of code, however, even these four lines can be annoying to type. Also, if we need to change the function, it can be annoying to go and update all of them. And everything will get even more complicated with more lines of code.

In order to resolve that we will create a function. Remember how I said that everything is an object in *R*? Here is where this is very useful. We can just store a function in an object.

Basically we will just use the logic we used in the `mutate()` and store it like we do with everything else.

```{r custom_function_mean}
# First we need to pick a name for the function. Since naming objects is not the scope of this article we will just pick something at random
my_awesome_function <- function(x){ # this tells R that it is a function and x is whatever we choose to include in there
  
  # The actual action that is done when calling this function
  ifelse(is.na(x), mean(x, na.rm = T), x)
}
```

Let's check and see if the results are the same.

```{r using_custom_functions_1}
df %>%
  mutate(x = my_awesome_function(x),
         y = my_awesome_function(y),
         z = my_awesome_function(z),
         q = my_awesome_function(q))
```

As you can see, the results are the same, with a lot less code written. This trick is also handy if you want to change the function in the future. All you need to do is change the original function and leave the rest code intact. Like this:

```{r custom_function_median}
my_awesome_function <- function(x){ 
  
  ifelse(is.na(x), median(x, na.rm = T), x)
}


df %>%
  mutate(x = my_awesome_function(x),
         y = my_awesome_function(y),
         z = my_awesome_function(z),
         q = my_awesome_function(q))
```

Please note that all you need to do if you want to use the `median()` and not the `mean()` is update one line of code, not four. Writing custom functions is a very powerful tool as it allows you to centralise your logic and make sure you are using the same operation across all variables that need it. 
  
Think of how easy would be to forget a `na.rm = TRUE` if you have 40 variables that require this operation, especially that you might not use this in one place in your project.

If you want, you can give this function more flexibility and add the operation we want to use as a parameter to our awesome function.

```{r custom_function_final}
my_awesome_function <- function(x, .func){
  ifelse(is.na(x), 
         .func(x, na.rm = T),
         x)
}


df %>%
  mutate(x = my_awesome_function(x, mean),
         y = my_awesome_function(y, mean),
         z = my_awesome_function(z, median),
         q = my_awesome_function(q, sum))
```

As you can see, this version of the custom function allows more flexibility in deciding what operation we want to use on our variables. However, we also need to take into account that this will require us to change the operation in multiple places, if we want to change the default. If we want to use different operation on different columns, this would be the best approach.

It all depends on your needs.

With this said, I would like to show you a code example that will recreate the graphs we had earlier and allow us to change the `x` and `y` axes and the colour grouping.

```{r custom_function_plot}
my_plot <- function(.data, x, y, colour, nudge){
  x <- enquo(x)
  y <- enquo(y)
  colour <- enquo(colour)
  
  ggplot(.data, mapping = aes(x = !!x, y = !!y, colour = !!colour)) +
    geom_boxplot() +
    geom_point() +
    geom_text(mapping = aes(label = car_names), hjust = 0, nudge_x = nudge, check_overlap = TRUE)
  
}
mtcars %>%
  mutate(car_names = rownames(mtcars),
         cyl = as.factor(cyl)) %>%
  # Our custom function
  my_plot(cyl, mpg, cyl, 0.05)
```

I hope that I managed to impress upon you the utility and importance of custom functions in any data analysis project in *R*, if you want to be able to update and maintain the code easily and relatively fast.

In this article I have only scratched the surface of what custom functions can do in *R* as the point of this article was to show you how you can write code that is easier to maintain and update. I plan on writing an article about functions and go into more detail, however, in the meantime, I would highly recommend checking out Hadley Wickham's book [Advanced R](http://adv-r.had.co.nz/Introduction.html). It is amazing!

Let me know if you found this article helpful in your data projects.
