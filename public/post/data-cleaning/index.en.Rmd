---
title: Data Cleaning
author: Vlad Aluas
date: '`r Sys.Date()`'
slug: data-cleaning
categories: ["Tutorial"]
tags: ["R", "tidyverse", "data cleaning"]
thumbnail: /images/code-logo.png
toc: true
---

</br>

> "Like families, tidy datasets are all alike but every messy dataset is messy in its own way." `r tufte::quote_footer('*--- Hadley Wickham, Chief Scientist at RStudio*')`

This article will try to show how we can link the structure of a dataset to it's meaning and make sure the data is showing what we want to show.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", collapse = FALSE)
```

------------------------------------------------------------------------

# Introduction

In my experience, a data analyst spends most of his or hers time on data cleaning and data validation. For quite a few companies, data cleaning means a series of manual tasks that take up a lot of time and require a lot of focus from the data analysts' part. I am sure most of my colleagues can confirm this, especially those who work in an organisation that is slow to change its ways or work with people who are too accustomed to their ways that they are reluctant to adopt new tools and methods.

Although as quoted at the beginning of this article, every dataset is messy in its own way, and common sense would suggest that the best way to clean it is to do it manually, I disagree. The are some ways in which one can automate the data cleaning process. Even if you do it partially is still a huge win.

I do not agree with a manual approach to repetitive tasks for several reasons. Here are the ones that I consider the most important:

-   The quality of the work can vary between individuals. Some people are more organised than others and that can decrease the chances of overlooking something
-   People get bored doing the same thing over and over again
-   Doing tasks that take a lot of time and little creativity, data cleaning can be like that, can lower motivation and the analysts will no longer try to improve the datasets or find useful insights into the data

I think it's in everyone's interest for the analysts to have free time to think of ways in which they can use the data to improve the company or the environment around them, not just make sure the data is there and it is correct, checking record by record. For me that used to be the most frustrating part of the job and it took me a lot of energy to find the motivation to start the work.

In the above section I have discussed why data cleaning might be problematic when done manually. In the next section I would like to go into a bit more detail about what is data cleaning and how we can automate some of the processes done by a data analyst so they can focus on analysis more than data manipulation.

# What is data cleaning and data wrangling

## Data cleaning

We will start with data cleaning as it is the more general term used by people and generally refers to a process through which the data quality is ensured. This can include but is not limited to:

-   ensuring the data format is correct (*e.g.* the dates have a standard format in the data source so the analysis software can detect it)
-   dealing with missing data
-   dealing with outliers

When dealing with these cases, analysts don't necessarily spend the time to analyse each observation individually, but they rather follow a predetermined set of rules, usually a general rule per variable. We can see some examples below:

-   changing the dates in a suitable format
-   replacing missing records either with a *mean*, *median* or *remove* them altogether
-   eliminating outliers from the analysis or separating them in a different dataset so they can be studied in detail

No matter which of these is encountered daily, there is no reason why this cannot be automated as we already have the logic for it, so we can program a computer to do the same.

## Data wrangling

Data wrangling is the process through which we manipulate the data so we can transform it to a format that is more suitable for our purposes. Data can come in a plethora of formats, however, when it comes to tabular data, which is the focus of this article, the format I found to be most useful and easiest to read and manipulate by most data software is a *Tidy Format* quoted at the beginning of this article(see [here](https://vita.had.co.nz/papers/tidy-data.pdf)).

We will see some interesting tools to use for data wrangling in the examples that follow.

# Data cleaning in practice

In the next section, we will discuss and go through a series of exercises that will allow us to clean and manipulate the data in a data source. For this we will use the `tidyverse` package from *R*. If you do not have it installed, you can do so by copying the following line into your console `install.packages("tidyverse")`.

Now for the actual analysis, we will start by activating it in the environment.

```{r libraries, message=FALSE}
library(tidyverse)
```


In order to play around with a dataset, we will use the *mtcars* dataset that comes with the `tidyverse` package. We can see the dataset using the code below:

```{r mtcars_head}
mtcars %>%      # Our Dataset
      head()    # First six results
```

If you are new to *R* and are not familiar with the `%>%` symbol used earlier, a good way to think of it is to consider that it links the actions attached to a dataset. It is to be read "take *dataset* then (%\>%)". In this case it would be "take *mtcars* then show the top 5 records".

Now that we have the dataset and the knowledge on how to read the code, let's start with actual data cleaning and wrangling.

### Mutate

As we can see, the row numbers in this data set are the car names. However, this is not very useful if we want to use the names in our analysis or group by name. So we will use a function that is very useful in data cleaning, `mutate()`. 

`mutate()` allows the users to either create a new column at the end of a dataset, which is quite useful, or change an existing column. We will use this to create a new column with the names of the cars.

```{r mutate_example}
mtcars %>%
  mutate(car_names = row.names(mtcars)) %>%
  head()
```

There, we have added a new column to the dataset. Now, what if we need to filter something out of the data set? 

Luckily the `tidyverse` is quite intuitive when it comes to naming functions and we can use the `filter()` function.

### Filter

Let's filter out from the dataset the cars with four cylinders (*cyl*).

```{r filter_cyl}
mtcars %>%
  filter(cyl != 4)
```

As we can see the *Datsun 710* that had 4 cylinders has disappeared from the first 6 records.

Of course, this function can also filter specific variables, and include multiple arguments. Let's try to filter just the cars that have 4 cylinders and more that 90 horse power (*hp*).

```{r filter_cyl_hp}
mtcars %>%
  filter(cyl == 4, hp > 90)
```

Please note that this function will not work if you DO NOT use a double `=` when trying to compare values. One `=` means attribution in *R*, which is why the function cannot accept it in a comparison.

If you want to manipulate missing data in a column you can use the following:

*   `filter(is.na(column_name))` filters the rows that have a missing values for the specified column 
*   `filter(!is.na(column_name))` filters the rows that DO NOT have a missing values for the specified column 

### Selecting columns

Maybe in some situations, you do not need to use the full dataset you have at your disposal. 

In this case, you can use a `select()` statement that will allow you to work with just the needed dataset. Let's select just two columns, *mpg* and *cyl*.

```{r select_example}
mtcars %>%
  select(mpg, cyl) %>%
  head()
```

Now we can select just the needed data without over exhausting our computing resources with unnecessary data.

What if we need to use multiple datasets and it would be good to have them all in one place? 

For that we can use a `join()` statement.

### Join

Joining two or more datasets is quite a straightforward process, however, I would like to take a moment and explain it a little bit for those who do not have experience with it and point out some possible aspects for which it would be good to watch out.

The process of joining two datasets requires a reference column with common values in both tables. The joining process will look in the column from the first set, will search the values for each row in the other set and will bring all the values associated with that row in the first set. Let's see a simple example below. We will create two dummy data sets so we can demonstrate this better.

```{r join_datasets}
letters <- cbind(ID = c(1, 2, 3, 3, 1, 2),
                 Value1 = c("A", "B", "C", "D", "E", "F")) %>%
           as_tibble()

numbers <- cbind(ID = c(1, 2, 3, 4, 5, 6),
                 Value2 = c("one", "two", "three", "four", "five", "six")) %>%
           as_tibble()
```

```{r join_letters}
letters
```

````{r join_numbers}
numbers
```

Now that we have the two datasets I would like to point out that, as you can see, the *ID* values in the *numbers* dataset are unique and each is associated with a different value. In the first set, they are not, each appearing twice. Now let's check the join. We will use the `left_join()` function as it is the most common.

```{r join_example_1}
letters %>%
  left_join(numbers, by = "ID")
```

As we can see, the values from *numbers* associated with a particular *ID* have been added to the *letters* table. Note that the values higher than three are missing because they do not have an associated *ID* in the first dataset. Now let's see what happens if we try to do it the other way around. Remember that we need the joined table to have unique *ID* values and here is why.

```{r join_example_2}
numbers %>%
  left_join(letters, by = "ID")
```

Please notice that the function created additional rows for each time it encountered the needed *ID*.

### Pivoting and Unpivoting Data

Another useful function for a data analyst is to be able to pivot and un-pivot data. There are a couple of methods to do this. The simplest is using two functions `pivot_longer()` and `pivot_wider()`. The first function gathers multiple columns in one (makes a table longer) and the other function creates new columns using a previous column. The functionality is similar to that of a pivot_table in Excel.

Let's take a look at it in the following examples:

#### Pivot wider

```{r pivot_wider}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg)
```

The function took the values from the **cyl** column, created new columns with those values and filled them with the values from the **mpg** column.

#### Pivot Longer

Pivot longer does the exact opposite. Let's see.

```{r pivot_longer}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg) %>%
  pivot_longer(cols = c("4", "6", "8"), names_to = "cyl", values_to = "mpg")

```

Now this created some extra rows, one for each extra column we previously created. We do not need three for each car considering that two of the are *NA's*. All we need to do in this situation is filter the data, eliminating the *NA* values.

```{r pivot_longer_filter}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg) %>%
  pivot_longer(cols = c("4", "6", "8"), names_to = "cyl", values_to = "mpg") %>%
  filter(!is.na(mpg)) # notice the ! before the function in order to use the negative of the function

```

And now we can use a select to rearrange the columns.

```{r multi_pivoted_mtcars}
mtcars %>%
  pivot_wider(names_from = cyl, values_from = mpg) %>%
  pivot_longer(cols = c("4", "6", "8"), names_to = "cyl", values_to = "mpg") %>%
  filter(!is.na(mpg)) %>%
  select(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) %>%
  head()
```

```{r initial_data}
mtcars %>%
  head()
```

# Conclusions

As we could see in this article, *R* is a tool that offers a lot of flexibility when it comes to data cleaning and data wrangling. Therefore I highly recommended that data analysts use this tool, or other automation tools, in their jobs as it will improve the data quality and it will reduce the time spent on frustrating or boring tasks.

I know it can be a bit of a headache at first since the learning curve for *R* can be a bit steep, however, I consider it is worth it. When I first started using *R* I was a bit intimidated by the fact that I did not know where to start implementing it into my job. Then I realised that it is enough to automate part of it, not everything at once and the results started to show immediately. All I needed to do was think of what were the steps I had to take in order to check something or create a report, and recreate those steps in *R*.

This is a very good first step. You will see that reports or tasks that can take hours or days can be done in a few seconds or minutes if you automate your work and let a script run.

Now that you have a lot more free time, you have the time to let me know how it went or if you encountered any problems.
